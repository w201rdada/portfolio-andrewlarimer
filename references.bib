
@article{KumarKnowledgeGraphbased2017e,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09222},
  primaryClass = {cs},
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/andrewlarimer/Zotero/storage/IQYTIRHH/Kumar et al. - 2017 - Towards a Knowledge Graph based Speech Interface.pdf;/Users/andrewlarimer/Zotero/storage/YAYSA24D/1705.html}
}

@article{VakhariaDesigningSpontaneousSpeech2013a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.4706},
  primaryClass = {cs},
  title = {Designing {{Spontaneous Speech Search Interface}} for {{Historical Archives}}},
  abstract = {Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information.},
  journal = {arXiv:1312.4706 [cs]},
  author = {Vakharia, Donna and Gibbs, Rachel},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/andrewlarimer/Zotero/storage/NQRN5CF4/Vakharia and Gibbs - 2013 - Designing Spontaneous Speech Search Interface for .pdf;/Users/andrewlarimer/Zotero/storage/J7G3VSZM/1312.html}
}

@article{KumarKnowledgeGraphbased2017d,
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{KumarKnowledgeGraphbased2017c,
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{KumarKnowledgeGraphbased2017b,
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{KumarKnowledgeGraphbased2017a,
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{VakhariaDesigningSpontaneousSpeech2013,
  title = {Designing {{Spontaneous Speech Search Interface}} for {{Historical Archives}}},
  abstract = {Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information.},
  journal = {arXiv:1312.4706 [cs]},
  author = {Vakharia, Donna and Gibbs, Rachel},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{KumarKnowledgeGraphbased2017,
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{KumarKnowledgeGraphbased2017f,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09222},
  primaryClass = {cs},
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/andrewlarimer/Zotero/storage/BI6Z7WSL/Kumar et al. - 2017 - Towards a Knowledge Graph based Speech Interface.pdf;/Users/andrewlarimer/Zotero/storage/8GZZ2J74/1705.html}
}

@article{kumar_towards_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09222},
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{vakharia_designing_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.4706},
  title = {Designing {{Spontaneous Speech Search Interface}} for {{Historical Archives}}},
  abstract = {Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information.},
  journal = {arXiv:1312.4706 [cs]},
  author = {Vakharia, Donna and Gibbs, Rachel},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{kumar_towards_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09222},
  title = {Towards a {{Knowledge Graph}} Based {{Speech Interface}}},
  abstract = {Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems.},
  journal = {arXiv:1705.09222 [cs]},
  author = {Kumar, Ashwini Jaya and Auer, S{\"o}ren and Schmidt, Christoph and {k{\"o}hler}, Joachim},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}


